{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSN Code Classification System using RAG and Knowledge Graphs\n",
    "\n",
    "## AI/ML Engineering Course Assignment\n",
    "\n",
    "**Author:** Santosh Kumar  \n",
    "**Date:** January 8, 2026\n",
    "\n",
    "---\n",
    "\n",
    "### System Overview\n",
    "\n",
    "This notebook implements an intelligent HSN (Harmonized System Nomenclature) Code Classification System combining:\n",
    "- Knowledge Graph construction for hierarchical relationships\n",
    "- RAG (Retrieval-Augmented Generation) for semantic search\n",
    "- Vector embeddings for similarity matching\n",
    "- Natural language query processing\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "User Query → Query Processor → Vector Search → Knowledge Graph → Disambiguation → Result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Introduction and Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Core libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Visualization libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "print(\"✓ ML libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Explore Dataset from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hsn_data_from_pdf(pdf_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract HSN data from PDF using pdfplumber for table extraction\n",
    "    \"\"\"\n",
    "    print(f\"Extracting data from PDF: {pdf_path}...\")\n",
    "    \n",
    "    all_tables = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        print(f\"Total pages: {len(pdf.pages)}\")\n",
    "        \n",
    "        for page_num, page in enumerate(pdf.pages, 1):\n",
    "            tables = page.extract_tables()\n",
    "            \n",
    "            if tables:\n",
    "                for table in tables:\n",
    "                    if table and len(table) > 0:\n",
    "                        all_tables.extend(table)\n",
    "    \n",
    "    if not all_tables:\n",
    "        raise ValueError(\"No tables found in PDF\")\n",
    "    \n",
    "    # Assume first row is header\n",
    "    header = all_tables[0]\n",
    "    data = all_tables[1:]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    \n",
    "    # Clean column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Convert HSN Code to numeric if possible\n",
    "    if 'HSN Code' in df.columns:\n",
    "        df['HSN Code'] = pd.to_numeric(df['HSN Code'], errors='coerce')\n",
    "    \n",
    "    # Remove rows with missing HSN codes\n",
    "    df = df.dropna(subset=['HSN Code'])\n",
    "    \n",
    "    print(f\"✓ Extracted {len(df)} records from PDF\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract data from PDF\n",
    "df = extract_hsn_data_from_pdf('hsn_data.pdf')\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nFirst 3 records:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Statistics:\\n\")\n",
    "print(f\"Total HSN Codes: {df['HSN Code'].nunique()}\")\n",
    "print(f\"Total Chapters: {df['ChapterNumber'].nunique()}\")\n",
    "print(f\"\\nChapter Distribution:\")\n",
    "print(df['ChapterNumber'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Data Processing and Enhancement\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract Hierarchical Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hierarchy(hsn_code: int) -> Dict[str, str]:\n",
    "    hsn_str = str(hsn_code).zfill(8)\n",
    "    \n",
    "    return {\n",
    "        'chapter': hsn_str[:2],\n",
    "        'heading': hsn_str[:4],\n",
    "        'subheading': hsn_str[:6],\n",
    "        'full_code': hsn_str\n",
    "    }\n",
    "\n",
    "df['hierarchy'] = df['HSN Code'].apply(extract_hierarchy)\n",
    "df['chapter'] = df['hierarchy'].apply(lambda x: x['chapter'])\n",
    "df['heading'] = df['hierarchy'].apply(lambda x: x['heading'])\n",
    "df['subheading'] = df['hierarchy'].apply(lambda x: x['subheading'])\n",
    "df['full_code'] = df['hierarchy'].apply(lambda x: x['full_code'])\n",
    "\n",
    "print(\"✓ Hierarchy extracted\")\n",
    "df[['HSN Code', 'chapter', 'heading', 'subheading', 'full_code']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create Enriched Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enriched_document(row: pd.Series) -> Dict:\n",
    "    hierarchy = extract_hierarchy(row['HSN Code'])\n",
    "    \n",
    "    doc = {\n",
    "        'hsn_code': row['HSN Code'],\n",
    "        'full_code': hierarchy['full_code'],\n",
    "        'description': row['Description'],\n",
    "        'trade_status': row['FinalHSN'],\n",
    "        'hierarchy': {\n",
    "            'chapter': {\n",
    "                'code': hierarchy['chapter'],\n",
    "                'description': row['Chapter_Description']\n",
    "            },\n",
    "            'heading': {\n",
    "                'code': hierarchy['heading'],\n",
    "                'description': row['Heading_Description']\n",
    "            },\n",
    "            'subheading': {\n",
    "                'code': hierarchy['subheading'],\n",
    "                'description': row['Subheading_Description']\n",
    "            },\n",
    "            'specific': {\n",
    "                'code': hierarchy['full_code'],\n",
    "                'description': row['Description']\n",
    "            }\n",
    "        },\n",
    "        'full_context': f\"\"\"HSN Code: {hierarchy['full_code']}\n",
    "Product: {row['Description']}\n",
    "Chapter {hierarchy['chapter']}: {row['Chapter_Description']}\n",
    "Heading {hierarchy['heading']}: {row['Heading_Description']}\n",
    "Subheading {hierarchy['subheading']}: {row['Subheading_Description']}\n",
    "Trade Status: {row['FinalHSN']}\"\"\"\n",
    "    }\n",
    "    \n",
    "    return doc\n",
    "\n",
    "enriched_documents = [create_enriched_document(row) for _, row in df.iterrows()]\n",
    "\n",
    "print(f\"✓ Created {len(enriched_documents)} enriched documents\")\n",
    "print(\"\\nSample enriched document:\")\n",
    "print(json.dumps(enriched_documents[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_hsn_data(documents: List[Dict]) -> Dict[str, any]:\n",
    "    validation_results = {\n",
    "        'total_documents': len(documents),\n",
    "        'unique_hsn_codes': len(set(doc['hsn_code'] for doc in documents)),\n",
    "        'unique_chapters': len(set(doc['hierarchy']['chapter']['code'] for doc in documents)),\n",
    "        'missing_descriptions': sum(1 for doc in documents if not doc['description']),\n",
    "        'valid_hierarchy': sum(1 for doc in documents if len(doc['full_code']) == 8)\n",
    "    }\n",
    "    \n",
    "    validation_results['data_quality_score'] = (\n",
    "        validation_results['valid_hierarchy'] / validation_results['total_documents'] * 100\n",
    "    )\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "validation = validate_hsn_data(enriched_documents)\n",
    "print(\"Data Validation Results:\\n\")\n",
    "for key, value in validation.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Knowledge Graph Construction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSNKnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.hsn_data = {}\n",
    "        \n",
    "    def build_graph(self, documents: List[Dict]):\n",
    "        for doc in documents:\n",
    "            hsn_code = doc['full_code']\n",
    "            hierarchy = doc['hierarchy']\n",
    "            \n",
    "            self.hsn_data[hsn_code] = doc\n",
    "            \n",
    "            chapter_code = hierarchy['chapter']['code']\n",
    "            heading_code = hierarchy['heading']['code']\n",
    "            subheading_code = hierarchy['subheading']['code']\n",
    "            \n",
    "            self._add_node(chapter_code, 'chapter', hierarchy['chapter']['description'])\n",
    "            self._add_node(heading_code, 'heading', hierarchy['heading']['description'])\n",
    "            self._add_node(subheading_code, 'subheading', hierarchy['subheading']['description'])\n",
    "            self._add_node(hsn_code, 'specific', doc['description'])\n",
    "            \n",
    "            self.graph.add_edge(chapter_code, heading_code, relation='contains')\n",
    "            self.graph.add_edge(heading_code, subheading_code, relation='contains')\n",
    "            self.graph.add_edge(subheading_code, hsn_code, relation='contains')\n",
    "            \n",
    "    def _add_node(self, code: str, level: str, description: str):\n",
    "        if code not in self.graph:\n",
    "            self.graph.add_node(code, level=level, description=description)\n",
    "            \n",
    "    def get_hierarchy_path(self, hsn_code: str) -> List[Dict]:\n",
    "        if hsn_code not in self.hsn_data:\n",
    "            return []\n",
    "            \n",
    "        doc = self.hsn_data[hsn_code]\n",
    "        hierarchy = doc['hierarchy']\n",
    "        \n",
    "        return [\n",
    "            {'level': 'chapter', 'code': hierarchy['chapter']['code'], \n",
    "             'description': hierarchy['chapter']['description']},\n",
    "            {'level': 'heading', 'code': hierarchy['heading']['code'], \n",
    "             'description': hierarchy['heading']['description']},\n",
    "            {'level': 'subheading', 'code': hierarchy['subheading']['code'], \n",
    "             'description': hierarchy['subheading']['description']},\n",
    "            {'level': 'specific', 'code': hsn_code, \n",
    "             'description': doc['description']}\n",
    "        ]\n",
    "        \n",
    "    def get_children(self, code: str) -> List[str]:\n",
    "        if code not in self.graph:\n",
    "            return []\n",
    "        return list(self.graph.successors(code))\n",
    "        \n",
    "    def get_siblings(self, hsn_code: str) -> List[str]:\n",
    "        if hsn_code not in self.graph:\n",
    "            return []\n",
    "            \n",
    "        parents = list(self.graph.predecessors(hsn_code))\n",
    "        if not parents:\n",
    "            return []\n",
    "            \n",
    "        parent = parents[0]\n",
    "        siblings = [child for child in self.graph.successors(parent) if child != hsn_code]\n",
    "        return siblings\n",
    "        \n",
    "    def get_statistics(self) -> Dict:\n",
    "        levels = defaultdict(int)\n",
    "        for node in self.graph.nodes():\n",
    "            level = self.graph.nodes[node]['level']\n",
    "            levels[level] += 1\n",
    "            \n",
    "        return {\n",
    "            'total_nodes': self.graph.number_of_nodes(),\n",
    "            'total_edges': self.graph.number_of_edges(),\n",
    "            'nodes_by_level': dict(levels),\n",
    "            'max_depth': max(len(list(nx.ancestors(self.graph, node))) \n",
    "                           for node in self.graph.nodes() if self.graph.out_degree(node) == 0)\n",
    "        }\n",
    "\n",
    "kg = HSNKnowledgeGraph()\n",
    "kg.build_graph(enriched_documents)\n",
    "\n",
    "print(\"✓ Knowledge Graph constructed\")\n",
    "print(\"\\nGraph Statistics:\")\n",
    "stats = kg.get_statistics()\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kg_subset(kg: HSNKnowledgeGraph, chapter_code: str = '40', max_nodes: int = 20):\n",
    "    subgraph_nodes = set([chapter_code])\n",
    "    \n",
    "    for node in list(subgraph_nodes):\n",
    "        if node in kg.graph:\n",
    "            children = list(kg.graph.successors(node))[:5]\n",
    "            subgraph_nodes.update(children)\n",
    "            \n",
    "    subgraph = kg.graph.subgraph(list(subgraph_nodes)[:max_nodes])\n",
    "    \n",
    "    plt.figure(figsize=(16, 10))\n",
    "    pos = nx.spring_layout(subgraph, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    color_map = {\n",
    "        'chapter': '#FF6B6B',\n",
    "        'heading': '#4ECDC4',\n",
    "        'subheading': '#45B7D1',\n",
    "        'specific': '#96CEB4'\n",
    "    }\n",
    "    \n",
    "    node_colors = [color_map[subgraph.nodes[node]['level']] for node in subgraph.nodes()]\n",
    "    node_sizes = [3000 if subgraph.nodes[node]['level'] == 'chapter' else \n",
    "                  2000 if subgraph.nodes[node]['level'] == 'heading' else\n",
    "                  1500 if subgraph.nodes[node]['level'] == 'subheading' else 1000\n",
    "                  for node in subgraph.nodes()]\n",
    "    \n",
    "    nx.draw_networkx_nodes(subgraph, pos, node_color=node_colors, \n",
    "                          node_size=node_sizes, alpha=0.9)\n",
    "    nx.draw_networkx_edges(subgraph, pos, edge_color='gray', \n",
    "                          arrows=True, arrowsize=20, width=2, alpha=0.6)\n",
    "    nx.draw_networkx_labels(subgraph, pos, font_size=8, font_weight='bold')\n",
    "    \n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                 markerfacecolor=color, markersize=10, label=level.capitalize())\n",
    "                      for level, color in color_map.items()]\n",
    "    plt.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "    \n",
    "    plt.title(f'HSN Knowledge Graph - Chapter {chapter_code} Hierarchy', fontsize=16, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "visualize_kg_subset(kg, '40', max_nodes=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Test Knowledge Graph Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hsn = '40011010'\n",
    "\n",
    "print(f\"Testing Knowledge Graph with HSN: {test_hsn}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Hierarchy Path:\")\n",
    "hierarchy_path = kg.get_hierarchy_path(test_hsn)\n",
    "for item in hierarchy_path:\n",
    "    print(f\"  {item['level'].upper()}: {item['code']} - {item['description']}\")\n",
    "\n",
    "print(\"\\n2. Sibling Codes:\")\n",
    "siblings = kg.get_siblings(test_hsn)\n",
    "for sibling in siblings[:5]:\n",
    "    if sibling in kg.hsn_data:\n",
    "        print(f\"  {sibling}: {kg.hsn_data[sibling]['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: RAG System Implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"✓ Model loaded: {embedding_model.get_sentence_embedding_dimension()} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSNVectorStore:\n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        \n",
    "    def create_embeddings(self, documents: List[Dict]):\n",
    "        self.documents = documents\n",
    "        \n",
    "        texts = [doc['full_context'] for doc in documents]\n",
    "        \n",
    "        print(f\"Creating embeddings for {len(texts)} documents...\")\n",
    "        self.embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
    "        \n",
    "        dimension = self.embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(self.embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"✓ Vector store created with {self.index.ntotal} vectors\")\n",
    "        \n",
    "    def search(self, query: str, top_k: int = 5) -> List[Tuple[Dict, float]]:\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        \n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        results = []\n",
    "        for idx, distance in zip(indices[0], distances[0]):\n",
    "            if idx < len(self.documents):\n",
    "                similarity_score = 1 / (1 + distance)\n",
    "                results.append((self.documents[idx], similarity_score))\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def get_statistics(self) -> Dict:\n",
    "        return {\n",
    "            'total_vectors': self.index.ntotal if self.index else 0,\n",
    "            'embedding_dimension': self.embeddings.shape[1] if self.embeddings is not None else 0,\n",
    "            'total_documents': len(self.documents)\n",
    "        }\n",
    "\n",
    "vector_store = HSNVectorStore(embedding_model)\n",
    "vector_store.create_embeddings(enriched_documents)\n",
    "\n",
    "print(\"\\nVector Store Statistics:\")\n",
    "vs_stats = vector_store.get_statistics()\n",
    "for key, value in vs_stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Test Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"natural rubber latex prevulcanised\"\n",
    "print(f\"Test Query: '{test_query}'\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = vector_store.search(test_query, top_k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i} (Similarity: {score:.4f}):\")\n",
    "    print(f\"HSN Code: {doc['full_code']}\")\n",
    "    print(f\"Description: {doc['description']}\")\n",
    "    print(f\"Chapter: {doc['hierarchy']['chapter']['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Intelligent Query Processing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Query Processor with Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSNQueryProcessor:\n",
    "    def __init__(self, vector_store: HSNVectorStore, knowledge_graph: HSNKnowledgeGraph):\n",
    "        self.vector_store = vector_store\n",
    "        self.kg = knowledge_graph\n",
    "        self.similarity_threshold = 0.3\n",
    "        \n",
    "    def process_query(self, query: str) -> Dict:\n",
    "        query_lower = query.lower().strip()\n",
    "        \n",
    "        hsn_pattern = r'\\b(\\d{8})\\b'\n",
    "        hsn_match = re.search(hsn_pattern, query)\n",
    "        if hsn_match:\n",
    "            return self._handle_direct_hsn_lookup(hsn_match.group(1))\n",
    "        \n",
    "        if any(keyword in query_lower for keyword in ['chapter', 'broad', 'category', 'classification']):\n",
    "            return self._handle_broad_category_query(query)\n",
    "        \n",
    "        return self._handle_product_query(query)\n",
    "    \n",
    "    def _handle_direct_hsn_lookup(self, hsn_code: str) -> Dict:\n",
    "        if hsn_code in self.kg.hsn_data:\n",
    "            doc = self.kg.hsn_data[hsn_code]\n",
    "            hierarchy_path = self.kg.get_hierarchy_path(hsn_code)\n",
    "            siblings = self.kg.get_siblings(hsn_code)\n",
    "            \n",
    "            return {\n",
    "                'query_type': 'direct_lookup',\n",
    "                'status': 'success',\n",
    "                'result': {\n",
    "                    'hsn_code': hsn_code,\n",
    "                    'description': doc['description'],\n",
    "                    'trade_status': doc['trade_status'],\n",
    "                    'hierarchy': hierarchy_path,\n",
    "                    'related_codes': [s for s in siblings[:5]]\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'query_type': 'direct_lookup',\n",
    "                'status': 'not_found',\n",
    "                'message': f'HSN Code {hsn_code} not found in database'\n",
    "            }\n",
    "    \n",
    "    def _handle_broad_category_query(self, query: str) -> Dict:\n",
    "        results = self.vector_store.search(query, top_k=10)\n",
    "        \n",
    "        chapters = {}\n",
    "        for doc, score in results:\n",
    "            chapter_code = doc['hierarchy']['chapter']['code']\n",
    "            if chapter_code not in chapters:\n",
    "                chapters[chapter_code] = {\n",
    "                    'code': chapter_code,\n",
    "                    'description': doc['hierarchy']['chapter']['description'],\n",
    "                    'count': 0\n",
    "                }\n",
    "            chapters[chapter_code]['count'] += 1\n",
    "        \n",
    "        return {\n",
    "            'query_type': 'broad_category',\n",
    "            'status': 'needs_refinement',\n",
    "            'message': 'Multiple chapters found. Please provide more specific product details.',\n",
    "            'chapters': list(chapters.values())\n",
    "        }\n",
    "    \n",
    "    def _handle_product_query(self, query: str) -> Dict:\n",
    "        results = self.vector_store.search(query, top_k=10)\n",
    "        \n",
    "        high_confidence_results = [(doc, score) for doc, score in results \n",
    "                                   if score >= self.similarity_threshold]\n",
    "        \n",
    "        if not high_confidence_results:\n",
    "            return {\n",
    "                'query_type': 'product_query',\n",
    "                'status': 'no_match',\n",
    "                'message': 'No matching HSN codes found. Please refine your query.',\n",
    "                'suggestions': [doc['description'] for doc, _ in results[:5]]\n",
    "            }\n",
    "        \n",
    "        if len(high_confidence_results) == 1:\n",
    "            doc, score = high_confidence_results[0]\n",
    "            return {\n",
    "                'query_type': 'product_query',\n",
    "                'status': 'single_match',\n",
    "                'confidence': score,\n",
    "                'result': {\n",
    "                    'hsn_code': doc['full_code'],\n",
    "                    'description': doc['description'],\n",
    "                    'trade_status': doc['trade_status'],\n",
    "                    'hierarchy': self.kg.get_hierarchy_path(doc['full_code'])\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return self._disambiguate_results(high_confidence_results, query)\n",
    "    \n",
    "    def _disambiguate_results(self, results: List[Tuple[Dict, float]], query: str) -> Dict:\n",
    "        unique_codes = {}\n",
    "        for doc, score in results:\n",
    "            hsn_code = doc['full_code']\n",
    "            if hsn_code not in unique_codes or score > unique_codes[hsn_code]['score']:\n",
    "                unique_codes[hsn_code] = {\n",
    "                    'hsn_code': hsn_code,\n",
    "                    'description': doc['description'],\n",
    "                    'score': score,\n",
    "                    'hierarchy': doc['hierarchy'],\n",
    "                    'trade_status': doc['trade_status']\n",
    "                }\n",
    "        \n",
    "        options = sorted(unique_codes.values(), key=lambda x: x['score'], reverse=True)[:5]\n",
    "        \n",
    "        return {\n",
    "            'query_type': 'product_query',\n",
    "            'status': 'disambiguation_needed',\n",
    "            'message': 'Multiple matching HSN codes found. Please select the most appropriate one:',\n",
    "            'options': options\n",
    "        }\n",
    "    \n",
    "    def format_response(self, result: Dict) -> str:\n",
    "        if result['status'] == 'success':\n",
    "            r = result['result']\n",
    "            output = f\"\"\"\\n{'='*70}\n",
    "HSN CODE DETAILS\n",
    "{'='*70}\n",
    "\n",
    "HSN Code: {r['hsn_code']}\n",
    "Description: {r['description']}\n",
    "Trade Status: {r['trade_status']}\n",
    "\n",
    "HIERARCHY:\n",
    "\"\"\"\n",
    "            for level in r['hierarchy']:\n",
    "                output += f\"  {level['level'].upper()}: {level['code']} - {level['description']}\\n\"\n",
    "            \n",
    "            if r.get('related_codes'):\n",
    "                output += f\"\\nRELATED CODES: {', '.join(r['related_codes'][:5])}\\n\"\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        elif result['status'] == 'single_match':\n",
    "            r = result['result']\n",
    "            output = f\"\"\"\\n{'='*70}\n",
    "HSN CODE MATCH (Confidence: {result['confidence']:.2%})\n",
    "{'='*70}\n",
    "\n",
    "HSN Code: {r['hsn_code']}\n",
    "Description: {r['description']}\n",
    "Trade Status: {r['trade_status']}\n",
    "\n",
    "HIERARCHY:\n",
    "\"\"\"\n",
    "            for level in r['hierarchy']:\n",
    "                output += f\"  {level['level'].upper()}: {level['code']} - {level['description']}\\n\"\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        elif result['status'] == 'disambiguation_needed':\n",
    "            output = f\"\"\"\\n{'='*70}\n",
    "{result['message']}\n",
    "{'='*70}\\n\"\"\"\n",
    "            \n",
    "            for i, option in enumerate(result['options'], 1):\n",
    "                output += f\"\"\"\\nOPTION {i} (Confidence: {option['score']:.2%}):\n",
    "  HSN Code: {option['hsn_code']}\n",
    "  Description: {option['description']}\n",
    "  Chapter: {option['hierarchy']['chapter']['description']}\n",
    "  Heading: {option['hierarchy']['heading']['description']}\n",
    "  Trade Status: {option['trade_status']}\\n\"\"\"\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        elif result['status'] == 'needs_refinement':\n",
    "            output = f\"\"\"\\n{'='*70}\n",
    "{result['message']}\n",
    "{'='*70}\\n\"\"\"\n",
    "            \n",
    "            for chapter in result['chapters']:\n",
    "                output += f\"\\nChapter {chapter['code']}: {chapter['description']} ({chapter['count']} matches)\\n\"\n",
    "            \n",
    "            return output\n",
    "        \n",
    "        else:\n",
    "            return f\"\\n{result['message']}\\n\"\n",
    "\n",
    "query_processor = HSNQueryProcessor(vector_store, kg)\n",
    "print(\"✓ Query processor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Test Cases and Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 1: Direct Product Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What is the HSN code for natural rubber latex?\"\n",
    "print(f\"Query: {query1}\")\n",
    "\n",
    "result1 = query_processor.process_query(query1)\n",
    "print(query_processor.format_response(result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 2: Specific Product Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"HSN code for prevulcanised rubber\"\n",
    "print(f\"Query: {query2}\")\n",
    "\n",
    "result2 = query_processor.process_query(query2)\n",
    "print(query_processor.format_response(result2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 3: Broad Category Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = \"Rubber products classification\"\n",
    "print(f\"Query: {query3}\")\n",
    "\n",
    "result3 = query_processor.process_query(query3)\n",
    "print(query_processor.format_response(result3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 4: Similar Products Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = \"Natural rubber latex\"\n",
    "print(f\"Query: {query4}\")\n",
    "\n",
    "result4 = query_processor.process_query(query4)\n",
    "print(query_processor.format_response(result4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case 5: Direct HSN Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = \"Tell me about HSN 40011010\"\n",
    "print(f\"Query: {query5}\")\n",
    "\n",
    "result5 = query_processor.process_query(query5)\n",
    "print(query_processor.format_response(result5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_queries = [\n",
    "    \"conveyor belts\",\n",
    "    \"synthetic rubber latex\",\n",
    "    \"transmission belts for machinery\",\n",
    "    \"vulcanised rubber thread\",\n",
    "    \"reclaimed rubber\"\n",
    "]\n",
    "\n",
    "print(\"Additional Test Cases:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for query in additional_queries:\n",
    "    print(f\"\\n\\nQuery: '{query}'\")\n",
    "    result = query_processor.process_query(query)\n",
    "    print(query_processor.format_response(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Performance Metrics and Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_system(query_processor, test_queries: List[str], num_runs: int = 10):\n",
    "    results = {\n",
    "        'query_times': [],\n",
    "        'retrieval_accuracy': [],\n",
    "        'disambiguation_rate': 0,\n",
    "        'direct_match_rate': 0\n",
    "    }\n",
    "    \n",
    "    status_counts = defaultdict(int)\n",
    "    \n",
    "    for query in test_queries:\n",
    "        start_time = time.time()\n",
    "        result = query_processor.process_query(query)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        results['query_times'].append(end_time - start_time)\n",
    "        status_counts[result['status']] += 1\n",
    "    \n",
    "    total_queries = len(test_queries)\n",
    "    results['disambiguation_rate'] = status_counts['disambiguation_needed'] / total_queries\n",
    "    results['direct_match_rate'] = (status_counts['success'] + status_counts['single_match']) / total_queries\n",
    "    results['avg_query_time'] = np.mean(results['query_times'])\n",
    "    results['status_distribution'] = dict(status_counts)\n",
    "    \n",
    "    return results\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the HSN code for natural rubber latex?\",\n",
    "    \"HSN code for prevulcanised rubber\",\n",
    "    \"Rubber products classification\",\n",
    "    \"Natural rubber latex\",\n",
    "    \"Tell me about HSN 40011010\",\n",
    "    \"conveyor belts\",\n",
    "    \"synthetic rubber\",\n",
    "    \"transmission belts\"\n",
    "]\n",
    "\n",
    "benchmark_results = benchmark_system(query_processor, test_queries)\n",
    "\n",
    "print(\"System Performance Metrics:\\n\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Average Query Time: {benchmark_results['avg_query_time']:.4f} seconds\")\n",
    "print(f\"Direct Match Rate: {benchmark_results['direct_match_rate']:.2%}\")\n",
    "print(f\"Disambiguation Rate: {benchmark_results['disambiguation_rate']:.2%}\")\n",
    "print(f\"\\nStatus Distribution:\")\n",
    "for status, count in benchmark_results['status_distribution'].items():\n",
    "    print(f\"  {status}: {count} ({count/len(test_queries):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(benchmark_results['query_times'], bins=20, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Query Time (seconds)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Query Response Time Distribution')\n",
    "axes[0].axvline(benchmark_results['avg_query_time'], color='red', linestyle='--', \n",
    "                label=f\"Avg: {benchmark_results['avg_query_time']:.4f}s\")\n",
    "axes[0].legend()\n",
    "\n",
    "status_data = benchmark_results['status_distribution']\n",
    "axes[1].bar(range(len(status_data)), list(status_data.values()), color='coral', edgecolor='black')\n",
    "axes[1].set_xticks(range(len(status_data)))\n",
    "axes[1].set_xticklabels(list(status_data.keys()), rotation=45, ha='right')\n",
    "axes[1].set_xlabel('Query Status')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Query Status Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: System Limitations and Future Improvements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Limitations\n",
    "\n",
    "1. **Limited Dataset**: Sample dataset contains only Chapter 40 (Rubber products)\n",
    "2. **Embedding Model**: Using lightweight model (all-MiniLM-L6-v2) for efficiency\n",
    "3. **No LLM Integration**: Classification based purely on embeddings without generative AI\n",
    "4. **Static Threshold**: Fixed similarity threshold may not work for all product types\n",
    "5. **No User Feedback Loop**: System doesn't learn from user corrections\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "1. **Expand Dataset**: Include all HSN chapters for comprehensive coverage\n",
    "2. **Advanced Embeddings**: Use domain-specific or larger embedding models\n",
    "3. **LLM Integration**: Add GPT/Claude for natural language understanding and generation\n",
    "4. **Dynamic Thresholds**: Implement adaptive similarity thresholds per category\n",
    "5. **Active Learning**: Incorporate user feedback to improve classification accuracy\n",
    "6. **Multi-language Support**: Add support for product descriptions in multiple languages\n",
    "7. **Image Integration**: Allow product image-based HSN code classification\n",
    "8. **Regulatory Updates**: Automatic synchronization with HSN code updates\n",
    "9. **Export Compliance**: Add trade restrictions and compliance checking\n",
    "10. **API Development**: Build REST API for enterprise integration\n",
    "\n",
    "### Scalability Considerations\n",
    "\n",
    "- **Vector Database**: Migrate to Pinecone/Weaviate for production scale\n",
    "- **Caching**: Implement Redis for frequent query caching\n",
    "- **Load Balancing**: Distribute requests across multiple instances\n",
    "- **Monitoring**: Add comprehensive logging and performance monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebook successfully implements an HSN Code Classification System combining:\n",
    "\n",
    "1. **Knowledge Graph**: Hierarchical representation of HSN codes with 4 levels (Chapter → Heading → Subheading → Specific)\n",
    "2. **Vector Store**: FAISS-based similarity search using sentence transformers\n",
    "3. **RAG System**: Retrieval-augmented approach for accurate classification\n",
    "4. **Query Processing**: Intelligent disambiguation and multi-strategy query handling\n",
    "\n",
    "### Key Achievements\n",
    "\n",
    "✓ Data processing pipeline with hierarchical enhancement  \n",
    "✓ Knowledge graph construction with NetworkX  \n",
    "✓ Vector embeddings and semantic search  \n",
    "✓ Intelligent query disambiguation  \n",
    "✓ Comprehensive test case validation  \n",
    "✓ Performance benchmarking and visualization  \n",
    "\n",
    "### Real-World Application\n",
    "\n",
    "This system can be deployed for:\n",
    "- Export documentation automation\n",
    "- Customs declaration assistance\n",
    "- Trade compliance verification\n",
    "- E-commerce product categorization\n",
    "- Supply chain management\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "Through this assignment, I gained hands-on experience with:\n",
    "- Building knowledge graphs from structured data\n",
    "- Implementing vector similarity search\n",
    "- Designing intelligent query processing systems\n",
    "- Handling ambiguous queries with disambiguation\n",
    "- Performance optimization and benchmarking\n",
    "- Real-world AI system architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "---\n",
    "\n",
    "1. **Sentence Transformers**: Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\n",
    "2. **FAISS**: Johnson, J., Douze, M., & Jégou, H. (2019). Billion-scale similarity search with GPUs\n",
    "3. **NetworkX**: Hagberg, A., Schult, D., & Swart, P. (2008). Exploring network structure, dynamics, and function using NetworkX\n",
    "4. **HSN Classification**: World Customs Organization - Harmonized System Nomenclature\n",
    "5. **RAG Systems**: Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
